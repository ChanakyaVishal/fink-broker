#!/usr/bin/env python
# Copyright 2019 AstroLab Software
# Author: Abhishek Chauhan
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Distribute the alerts to users

1. Use the Alert data that is stored in the Science database (HBase)
2. Apply user defined filters
3. Serialize into Avro
3. Publish to Kafka Topic(s)
"""
from pyspark.sql import DataFrame

import argparse
import json
import time
import os

from fink_broker.parser import getargs
from fink_broker.sparkUtils import init_sparksession
from fink_broker.distributionUtils import get_kafka_df, update_status_in_hbase
from fink_broker.distributionUtils import get_distribution_offset
from fink_broker.distributionUtils import group_df_into_struct
from fink_broker.hbaseUtils import flattenstruct
from fink_broker.filters import filter_df_using_xml, apply_user_defined_filters
from fink_broker.loggingUtils import get_fink_logger, inspect_application
from fink_broker.slackUtils import get_api_token, send_slack_alerts

from userfilters.leveltwo import filter_leveltwo_names

def main():
    parser = argparse.ArgumentParser(description=__doc__)
    args = getargs(parser)

    # Initialise Spark session
    spark = init_sparksession(name="distribute", shuffle_partitions=2)

    # The level here should be controlled by an argument.
    logger = get_fink_logger(spark.sparkContext.appName, args.log_level)

    # debug statements
    inspect_application(logger)

    # Read the catalog file generated by raw2science
    science_db_catalog = args.science_db_catalog
    with open(science_db_catalog) as f:
        catalog = json.load(f)

    # Define variables
    min_timestamp = 100     # set a default
    t_end = 1577836799      # some default value

    # get distribution offset
    min_timestamp = get_distribution_offset(
        args.checkpointpath_dist, args.startingOffset_dist)

    # Get topic name to publish on
    topic = args.distribution_topic
    broker_list = args.distribution_servers

    # Run distribution for (args.exit_after) seconds
    if args.exit_after is not None:
        t_end = time.time() + args.exit_after
        exit_after = True
    else:
        exit_after = False

    # Start the distribution service
    while(not exit_after or time.time() < t_end):
        """Keep scanning the HBase for new records in a loop
        """
        # Scan the HBase till current time
        max_timestamp = int(round(time.time() * 1000))  # time in ms

        # Read Hbase within timestamp range
        df = spark.read\
            .option("catalog", catalog)\
            .option("minStamp", min_timestamp)\
            .option("maxStamp", max_timestamp)\
            .format("org.apache.spark.sql.execution.datasources.hbase")\
            .load()

        # Keep records that haven't been distributed
        df = df.filter("status!='distributed'")

        # Send out slack alerts
        api_token = get_api_token()
        if api_token:
            slack_cols = [
                "objectId", "candidate_ra",
                "candidate_dec", "cross_match_alerts_per_batch"]
            send_slack_alerts(df.select(slack_cols), args.slack_channels)

        # Apply additional filters (user defined xml)
        if args.distribution_rules_xml:
            df = filter_df_using_xml(df, args.distribution_rules_xml)

        # create a nested dataframe similar to the original ztf dataframe
        df_nested = group_df_into_struct(df, "candidate", "objectId")
        df_nested = group_df_into_struct(df_nested, "prv_candidates", "objectId")
        df_nested = group_df_into_struct(df_nested, "cutoutTemplate", "objectId")
        df_nested = group_df_into_struct(df_nested, "cutoutScience", "objectId")
        df_nested = group_df_into_struct(df_nested, "cutoutDifference", "objectId")

        # Apply level two filters
        df_nested = apply_user_defined_filters(df_nested, filter_leveltwo_names)

        # Persist df to memory to materialize changes
        df_nested.persist()

        # Get the DataFrame for publishing to Kafka (avro serialized)
        df_kafka = get_kafka_df(df_nested, args.distribution_schema)

        # Ensure that the topic(s) exist on the Kafka Server)
        df_kafka\
            .write\
            .format("kafka")\
            .option("kafka.bootstrap.servers", broker_list)\
            .option("kafka.security.protocol", "SASL_PLAINTEXT")\
            .option("kafka.sasl.mechanism", "SCRAM-SHA-512")\
            .option("topic", topic)\
            .save()

        # Update the status in Hbase and commit checkpoint to file
        update_status_in_hbase(
            df, args.science_db_name, "objectId",
            args.checkpointpath_dist, max_timestamp)

        # update min_timestamp for next iteration
        min_timestamp = max_timestamp

        # free the memory
        df_nested.unpersist()

        # Wait for some time before another loop
        time.sleep(1)


if __name__ == "__main__":
    main()
